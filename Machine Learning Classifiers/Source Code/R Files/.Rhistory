barplot(diabetes_data$V5)
barplot(diabetes_data$V6)
barplot(diabetes_data$V7)
barplot(diabetes_data$V8)
diabetes_data<-read.csv(fileName,header = FALSE)
diabetes_data.csv
attach(diabetes_data) 
args <- commandArgs(TRUE)
fileName <- args[1]
diabetes_data<-read.csv(fileName,header = FALSE)
paste("The correlation between attribute1 and the class variable is: ",cor(diabetes_data$V1,diabetes_data$V9),sep="")
paste("The correlation between attribute2 and the class variable is: ",cor(diabetes_data$V2,diabetes_data$V9),sep="")
paste("The correlation between attribute3 and the class variable is: ",cor(diabetes_data$V3,diabetes_data$V9),sep="")
paste("The correlation between attribute4 and the class variable is: ",cor(diabetes_data$V4,diabetes_data$V9),sep="")
paste("The correlation between attribute5 and the class variable is: ",cor(diabetes_data$V5,diabetes_data$V9),sep="")
paste("The correlation between attribute6 and the class variable is: ",cor(diabetes_data$V6,diabetes_data$V9),sep="")
paste("The correlation between attribute7 and the class variable is: ",cor(diabetes_data$V7,diabetes_data$V9),sep="")
paste("The correlation between attribute8 and the class variable is: ",cor(diabetes_data$V8,diabetes_data$V9),sep="")
args <- commandArgs(TRUE)
fileName <- args[1]
diabetes_data<-read.csv(fileName,header = FALSE)
maxCorrelationVal<-0
maxFirstAttributeIndex<-0
maxSecondAttributeIndex<-0
corrVal<-0
for (i in 1:7)
{
a<-i+1
for (j in a:8)
{
corrVal<-cor(diabetes_data[[i]],diabetes_data[[j]])
if(corrVal>maxCorrelationVal)
{
maxCorrelationVal<-corrVal
maxFirstAttributeIndex<-i
maxSecondAttributeIndex<-j
}
}
}
paste("Maximum correlation value is: ",maxCorrelationVal,sep="")
paste("The attributes are: ","V",maxFirstAttributeIndex," and ","V",maxSecondAttributeIndex,sep="")
args <- commandArgs(TRUE)
fileName <- args[1]
diabetes_data<-read.csv(fileName,header = FALSE)
attach(diabetes_data)
par(mfrow=c(3,3))
hist(diabetes_data$V1)
hist(diabetes_data$V2)
hist(diabetes_data$V3)
hist(diabetes_data$V4)
hist(diabetes_data$V5)
hist(diabetes_data$V6)
hist(diabetes_data$V7)
hist(diabetes_data$V8)
args <- commandArgs(TRUE)
args <- commandArgs(TRUE)
fileName <- args[1]
diabetes_data<-read.csv(fileName,header = FALSE)
dataset1 = read.csv("C:\\Users\\navan\\Desktop\\Machine Learning Homework 2\\pima-indians-diabetes.data.csv")
library(e1071)
dataset1 = read.csv("C:\\Users\\navan\\Desktop\\Machine Learning Homework 2\\pima-indians-diabetes.data.csv")
str(dataset1)
i = 1
sum = 0
count = 0
kernels = c('','linear','polynomial','radial','sigmoid')
flag =0
for(kernel1 in kernels){
for (i  in 1:10)
{
split_data<- sample(nrow(dataset1), size = nrow(dataset1)* 0.9)
Train_Data<-dataset1[split_data,]
TestData <- dataset1[-split_data,]
if(flag==0){
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8])
predicted_labels <- predict(model_svm,TestData[,1:8])
}
else{
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8],kernal=kernel1)
predicted_labels <- predict(model_svm,TestData[,1:8])
}
table(predicted_labels,TestData[,9])
result <- sum(predicted_labels == TestData$X1)/length(predicted_labels)
#cat("Experiment ",i,": Accuracy is",result*100,"% \n")
sum = sum + result*100
count = count + 1
}
avg = sum/count
if(flag==0){
cat("Overall Accuracy Default Kernel is =",avg,"%\n")
flag =1
}
else{
cat("Overall Accuracy =",avg,"% kernel =",kernel1,"\n")
}
}
library(e1071)
dataset1 = read.csv("C:\\Users\\navan\\Desktop\\Machine Learning Homework 2\\pima-indians-diabetes.data.csv")
str(dataset1)
i = 1
sum = 0
count = 0
kernels = c('','linear','polynomial','radial','sigmoid')
flag =0
for(kernel1 in kernels){
for (i  in 1:10)
{
split_data<- sample(nrow(dataset1), size = nrow(dataset1)* 0.9)
Train_Data<-dataset1[split_data,]
TestData <- dataset1[-split_data,]
if(flag==0){
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8])
predicted_labels <- predict(model_svm,TestData[,1:8])
}
else{
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8],kernal=kernel1)
predicted_labels <- predict(model_svm,TestData[,1:8])
}
table(predicted_labels,TestData[,9])
result <- sum(predicted_labels == TestData$X1)/length(predicted_labels)
#cat("Experiment ",i,": Accuracy is",result*100,"% \n")
sum = sum + result*100
count = count + 1
}
avg = sum/count
if(flag==0){
cat("Overall Accuracy Default Kernel is =",avg,"%\n")
flag =1
}
else{
cat("Overall Accuracy =",avg,"% kernel =",kernel1,"\n")
}
}
library(e1071)
dataset1 = read.csv("C:\\Users\\navan\\Desktop\\Machine Learning Homework 2\\pima-indians-diabetes.data.csv")
str(dataset1)
i = 1
sum = 0
count = 0
kernels = c('','linear','polynomial','radial','sigmoid')
flag =0
for(kernel1 in kernels){
for (i  in 1:10)
{
split_data<- sample(nrow(dataset1), size = nrow(dataset1)* 0.9)
Train_Data<-dataset1[split_data,]
TestData <- dataset1[-split_data,]
if(flag==0){
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8])
predicted_labels <- predict(model_svm,TestData[,1:8])
}
else{
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8],kernal=kernel1)
predicted_labels <- predict(model_svm,TestData[,1:8])
}
table(predicted_labels,TestData[,9])
result <- sum(predicted_labels == TestData$X1)/length(predicted_labels)
#cat("Experiment ",i,": Accuracy is",result*100,"% \n")
sum = sum + result*100
count = count + 1
}
avg = sum/count
if(flag==0){
cat("Overall Accuracy Default Kernel is =",avg,"%\n")
flag =1
}
else{
cat("Overall Accuracy =",avg,"% kernel =",kernel1,"\n")
}
}
library(e1071)
dataset1 = read.csv("C:\\Users\\navan\\Desktop\\Machine Learning Homework 2\\pima-indians-diabetes.data.csv")
str(dataset1)
i = 1
sum = 0
count = 0
kernels = c('','linear','polynomial','radial','sigmoid')
flag =0
for(kernel1 in kernels){
for (i  in 1:10)
{
split_data<- sample(nrow(dataset1), size = nrow(dataset1)* 0.9)
Train_Data<-dataset1[split_data,]
TestData <- dataset1[-split_data,]
if(flag==0){
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8])
predicted_labels <- predict(model_svm,TestData[,1:8])
}
else{
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8],kernal=kernel1)
predicted_labels <- predict(model_svm,TestData[,1:8])
}
table(predicted_labels,TestData[,9])
result <- sum(predicted_labels == TestData$X1)/length(predicted_labels)
#cat("Experiment ",i,": Accuracy is",result*100,"% \n")
sum = sum + result*100
count = count + 1
}
avg = sum/count
if(flag==0){
cat("Overall Accuracy Default Kernel is =",avg,"%\n")
flag =1
}
else{
cat("Overall Accuracy =",avg,"% kernel =",kernel1,"\n")
}
}
count = count + 1
library(e1071)
dataset1 = read.csv("C:\\Users\\navan\\Desktop\\Machine Learning Homework 2\\pima-indians-diabetes.data.csv")
str(dataset1)
i = 1
sum = 0
count = 0
kernels = c('','linear','polynomial','radial','sigmoid')
flag =0
for(kernel1 in kernels){
for (i  in 1:10)
{
split_data<- sample(nrow(dataset1), size = nrow(dataset1)* 0.9)
Train_Data<-dataset1[split_data,]
TestData <- dataset1[-split_data,]
if(flag==0){
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8])
predicted_labels <- predict(model_svm,TestData[,1:8])
}
else{
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8],kernal=kernel1)
predicted_labels <- predict(model_svm,TestData[,1:8])
}
table(predicted_labels,TestData[,9])
result <- sum(predicted_labels == TestData$X1)/length(predicted_labels)
#cat("Experiment ",i,": Accuracy is",result*100,"% \n")
sum = sum + result*100
count = count + 1
}
avg = sum/count
if(flag==0){
cat("Overall Accuracy Default Kernel is =",avg,"%\n")
flag =1
}
else{
cat("Overall Accuracy =",avg,"% kernel =",kernel1,"\n")
}
}
library(e1071)
dataset1 = read.csv("C:\\Users\\navan\\Desktop\\Machine Learning Homework 2\\pima-indians-diabetes.data.csv")
str(dataset1)
i = 1
sum = 0
count = 0
kernels = c('','linear','polynomial','radial','sigmoid')
flag =0
for(kernel1 in kernels){
for (i  in 1:10)
{
split_data<- sample(nrow(dataset1), size = nrow(dataset1)* 0.9)
Train_Data<-dataset1[split_data,]
TestData <- dataset1[-split_data,]
if(flag==0){
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8])
predicted_labels <- predict(model_svm,TestData[,1:8])
}
else{
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8],kernal=kernel1)
predicted_labels <- predict(model_svm,TestData[,1:8])
}
table(predicted_labels,TestData[,9])
result <- sum(predicted_labels == TestData$X1)/length(predicted_labels)
#cat("Experiment ",i,": Accuracy is",result*100,"% \n")
sum = sum + result*100
count = count + 1
}
avg = sum/count
if(flag==0){
cat("Overall Accuracy Default Kernel is =",avg,"%\n")
flag =1
}
else{
cat("Overall Accuracy =",avg,"% kernel =",kernel1,"\n")
}
}
library(e1071)
dataset1 = read.csv("C:\\Users\\navan\\Desktop\\Machine Learning Homework 2\\pima-indians-diabetes.data.csv")
str(dataset1)
i = 1
sum = 0
count = 0
kernels = c('','linear','polynomial','radial','sigmoid')
flag =0
for(kernel1 in kernels){
for (i  in 1:10)
{
split_data<- sample(nrow(dataset1), size = nrow(dataset1)* 0.9)
Train_Data<-dataset1[split_data,]
TestData <- dataset1[-split_data,]
if(flag==0){
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8])
predicted_labels <- predict(model_svm,TestData[,1:8])
}
else{
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8],kernal=kernel1)
predicted_labels <- predict(model_svm,TestData[,1:8])
}
table(predicted_labels,TestData[,9])
result <- sum(predicted_labels == TestData$X1)/length(predicted_labels)
#cat("Experiment ",i,": Accuracy is",result*100,"% \n")
sum = sum + result*100
count = count + 1
}
avg = sum/count
if(flag==0){
cat("Overall Accuracy Default Kernel is =",avg,"%\n")
flag =1
}
else{
cat("Overall Accuracy =",avg,"% kernel =",kernel1,"\n")
}
}
library(e1071)
dataset1 = read.csv("C:\\Users\\navan\\Desktop\\Machine Learning Homework 2\\pima-indians-diabetes.data.csv")
str(dataset1)
i = 1
sum = 0
count = 0
kernels = c('','linear','polynomial','radial','sigmoid')
flag =0
for(kernel1 in kernels){
for (i  in 1:10)
{
split_data<- sample(nrow(dataset1), size = nrow(dataset1)* 0.9)
Train_Data<-dataset1[split_data,]
TestData <- dataset1[-split_data,]
if(flag==0){
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8])
predicted_labels <- predict(model_svm,TestData[,1:8])
}
else{
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8],kernal=kernel1)
predicted_labels <- predict(model_svm,TestData[,1:8])
}
table(predicted_labels,TestData[,9])
result <- sum(predicted_labels == TestData$X1)/length(predicted_labels)
#cat("Experiment ",i,": Accuracy is",result*100,"% \n")
sum = sum + result*100
count = count + 1
}
avg = sum/count
if(flag==0){
cat("Overall Accuracy Default Kernel is =",avg,"%\n")
flag =1
}
else{
cat("Overall Accuracy =",avg,"% kernel =",kernel1,"\n")
}
}
library(e1071)
dataset1 = read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data")
str(dataset1)
i = 1
sum = 0
count = 0
kernels = c('','linear','polynomial','radial','sigmoid')
flag =0
for(kernel1 in kernels){
for (i  in 1:10)
{
split_data<- sample(nrow(dataset1), size = nrow(dataset1)* 0.9)
Train_Data<-dataset1[split_data,]
TestData <- dataset1[-split_data,]
if(flag==0){
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8])
predicted_labels <- predict(model_svm,TestData[,1:8])
}
else{
model_svm <- svm(as.factor(Train_Data[,9]) ~ ., data = Train_Data[,1:8],kernal=kernel1)
predicted_labels <- predict(model_svm,TestData[,1:8])
}
table(predicted_labels,TestData[,9])
result <- sum(predicted_labels == TestData$X1)/length(predicted_labels)
#cat("Experiment ",i,": Accuracy is",result*100,"% \n")
sum = sum + result*100
count = count + 1
}
avg = sum/count
if(flag==0){
cat("Overall Accuracy Default Kernel is =",avg,"%\n")
flag =1
}
else{
cat("Overall Accuracy =",avg,"% kernel =",kernel1,"\n")
}
}
install.packages("e1071")
dataset <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data")
library("e1071")
n <- nrow(dataset)
sum=0.00;
for(i in 1:10){
trainRow <- sort(sample(1:n, floor(n*0.9)))
dataset.train <- dataset[trainRow,]
dataset.test <- dataset[-trainRow,]
m <- naiveBayes(as.factor(X1) ~ ., data = dataset.train, type="raw")
pred <- predict(m, dataset.test[,-9])
accuracy <- (mean(pred == t(dataset.test[9])))*100;
print(accuracy)
sum=sum+accuracy
}
overallAcc <- sum/10;
print(overallAcc)
install.packages("e1071")
dataset <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data")
library("e1071")
n <- nrow(dataset)
sum=0.00;
for(i in 1:10){
trainRow <- sort(sample(1:n, floor(n*0.9)))
dataset.train <- dataset[trainRow,]
dataset.test <- dataset[-trainRow,]
m <- naiveBayes(as.factor(X1) ~ ., data = dataset.train, type="raw")
pred <- predict(m, dataset.test[,-9])
accuracy <- (mean(pred == t(dataset.test[9])))*100;
print(accuracy)
sum=sum+accuracy
}
overallAcc <- sum/10;
print(overallAcc)
install.packages("class")
dataset <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data")
library("class")
n <- nrow(dataset)
kVal=c(3,5,7,9,11)
for(j in 1:5){
sum=0.00;
for(i in 1:10){
trainRow <- sort(sample(1:n, floor(n*0.9)))
dataset.train <- dataset[trainRow,]
dataset.test <- dataset[-trainRow,]
pred <- knn(dataset.train[,-9], dataset.test[,-9], dataset.train[,9], k = kVal[j], prob=TRUE)
accuracy <- (mean(pred == t(dataset.test[9])))*100
sum=sum+accuracy
}
overallAcc <- sum/10
print (kVal[j])
print(overallAcc)
}
install.packages("class")
dataset <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data")
library("class")
n <- nrow(dataset)
kVal=c(3,5,7,9,11)
for(j in 1:5){
sum=0.00;
for(i in 1:10){
trainRow <- sort(sample(1:n, floor(n*0.9)))
dataset.train <- dataset[trainRow,]
dataset.test <- dataset[-trainRow,]
pred <- knn(dataset.train[,-9], dataset.test[,-9], dataset.train[,9], k = kVal[j], prob=TRUE)
accuracy <- (mean(pred == t(dataset.test[9])))*100
sum=sum+accuracy
}
overallAcc <- sum/10
print (kVal[j])
print(overallAcc)
}
dataset <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data")
str(dataset)
# Histogram and Bar plot of all Attributes
hist(dataset$X6)
barplot(dataset$X6)
hist(dataset$X148)
barplot(dataset$X148)
hist(dataset$X72)
barplot(dataset$X72)
hist(dataset$X35)
barplot(dataset$X35)
hist(dataset$X0)
barplot(dataset$X0)
hist(dataset$X33.6)
barplot(dataset$X33.6)
hist(dataset$X0.627)
barplot(dataset$X0.627)
hist(dataset$X50)
barplot(dataset$X50)
#Correlation with Class variable
cor(dataset$X1,dataset$X6)
cor(dataset$X1,dataset$X148)
cor(dataset$X1,dataset$X72)
cor(dataset$X1,dataset$X35)
cor(dataset$X1,dataset$X0)
cor(dataset$X1,dataset$X33.6)
cor(dataset$X1,dataset$X0.627)
cor(dataset$X1,dataset$X50)
#Correlation with each attribute
max=0.0;
att1=1;
att2=1;
for(i in 1:8){
for(j in i:8){
if(i!=j){
tmp=cor(dataset[,i],dataset[,j])
if(tmp>max){
max=tmp;
att1=i;
att2=j;
}
}
}
}
max
colnames(dataset)[att1]
colnames(dataset)[att2]
